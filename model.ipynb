{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1da0d3-1900-4421-b0a5-29e3ac769653",
   "metadata": {},
   "source": [
    "## DATA COLLECTION AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c15d419-d224-4336-b90b-c3511ff99c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# libraries for manipulations\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026b9831-741d-4b3c-aeea-f0e7ade103a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the datasets for operations\n",
    "w1_data = pd.read_csv('water_dataset.csv')\n",
    "d1_data = pd.read_csv('crop_dataset.csv')\n",
    "d2_data = pd.read_csv('rainfall_soil_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef8d6a7-21f7-48f9-9b50-edaefc305c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [w1_data, d1_data, d2_data]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "396db285-ceb0-4aab-965e-5e1493be84df",
   "metadata": {},
   "source": [
    "# Read the CSV file\n",
    "w1_data = pd.read_csv('water_dataset.csv')\n",
    "\n",
    "# Iterate over columns\n",
    "for column in w1_data.columns:\n",
    "    # Check if the column has numeric data\n",
    "    if pd.api.types.is_numeric_dtype(w1_data[column]):\n",
    "        # Fill empty spaces with the mean of the column\n",
    "        w1_data[column].fillna(w1_data[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Fill empty spaces with NA for non-numeric columns\n",
    "        w1_data[column].fillna('NA', inplace=True)\n",
    "\n",
    "# Now w1_data has empty spaces filled according to the specified conditions\n",
    "w1_data.to_csv('water_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0771c621-c741-4716-ac28-4c0a7321af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = w1_data\n",
    "for column in data.columns:\n",
    "    # Check if the column has numeric data\n",
    "    if pd.api.types.is_numeric_dtype(data[column]):\n",
    "        # Fill empty spaces with the mean of the column\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "    else:\n",
    "        # Fill empty spaces with NA for non-numeric columns\n",
    "        data[column].fillna('NA', inplace=True)\n",
    "w1_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9128aa-df47-456e-9dc0-29409b2ad56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d1_data\n",
    "for column in data.columns:\n",
    "    if pd.api.types.is_numeric_dtype(data[column]):\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "    else:\n",
    "        data[column].fillna('NA', inplace=True)\n",
    "d1_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd415c8-a91a-42d4-8d7c-8436dcd9dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2_data\n",
    "for column in data.columns:\n",
    "    if pd.api.types.is_numeric_dtype(data[column]):\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "    else:\n",
    "        data[column].fillna('NA', inplace=True)\n",
    "d2_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debafe53-03f2-4442-b787-29e85aea4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features in a dataset stored\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# computing the minimum and maximum values for each feature\n",
    "numeric_columns = w1_data.select_dtypes(include=['number']).columns # only operating on the features who are numeric in nature\n",
    "w1_data_normalized = scaler.fit_transform(w1_data[numeric_columns]) # transformation of the numeric feature\n",
    "\n",
    "numeric_columns = d1_data.select_dtypes(include=['number']).columns\n",
    "d1_data_normalized = scaler.fit_transform(d1_data[numeric_columns])\n",
    "\n",
    "numeric_columns = d2_data.select_dtypes(include=['number']).columns\n",
    "d2_data_normalized = scaler.fit_transform(d2_data[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ca6b8b-ede4-4799-b6d1-572541749e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical Values \n",
    "\n",
    "# Perform one-hot encoding for categorical columns\n",
    "w1_data_encoded = pd.get_dummies(w1_data, columns=[\"rural_or_urban\", \"rural_or_urban\", \"ref_water_body_type_id_name\", \"water_body_loc_name\", \"water_body_ownership_name\", \"water_body_nature_name\"])\n",
    "d1_data_encoded = pd.get_dummies(d1_data, columns=[\"Crop\", \"Season\"])\n",
    "d2_data_encoded = pd.get_dummies(d2_data, columns=[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69b5f7-55e4-4cfb-a7ef-861c54525c74",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243327bf-10f3-47ba-b94c-fc89d93d4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering new features\n",
    "\n",
    "w1_data['water_utilization_efficiency'] = w1_data['water_spread_area_of_water_body'] * w1_data['storage_capacity_water_body_present']\n",
    "d1_data['yield_per_area'] = d1_data['Yield'] / d1_data['Area']\n",
    "d2_data['soil_fertility_index'] = d2_data['N'] + d2_data['P'] + d2_data['K']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6062aefd-7767-4670-bd12-82fee01b3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select relevant features\n",
    "\n",
    "relevant_features_w1 = w1_data[['ref_water_body_type_id_name', 'water_body_ownership_name', 'water_body_nature_name', 'water_spread_area_of_water_body', 'storage_capacity_water_body_present', 'water_utilization_efficiency']]\n",
    "relevant_features_d1 = d1_data[['Crop', 'Annual_Rainfall', 'Yield', 'yield_per_area']]\n",
    "relevant_features_d2 = d2_data[['temperature', 'humidity', 'ph', 'rainfall', 'soil_fertility_index']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b90cc3-dbed-40c9-a208-bac04507051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# Initialize Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to selected features in each dataset\n",
    "# only operating on the features who are numeric in nature\n",
    "numeric_columns = relevant_features_w1.select_dtypes(include=['number']).columns \n",
    "scaled_features_w1 = scaler.fit_transform(relevant_features_w1[numeric_columns])\n",
    "\n",
    "numeric_columns = relevant_features_d1.select_dtypes(include=['number']).columns\n",
    "scaled_features_d1 = scaler.fit_transform(relevant_features_d1[numeric_columns])\n",
    "\n",
    "numeric_columns = relevant_features_d2.select_dtypes(include=['number']).columns\n",
    "scaled_features_d2 = scaler.fit_transform(relevant_features_d2[numeric_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2173a1-4b3d-4700-bb69-0fae4bfd005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Initialize SelectKBest with f_regression scoring function\n",
    "selector = SelectKBest(score_func=f_regression, k=2)\n",
    "\n",
    "# Apply feature selection to identify top 2 features in each dataset\n",
    "selected_features_w1 = selector.fit_transform(scaled_features_w1, w1_data['water_utilization_efficiency'])\n",
    "selected_features_d1 = selector.fit_transform(scaled_features_d1, d1_data['yield_per_area'])\n",
    "selected_features_d2 = selector.fit_transform(scaled_features_d2, d2_data['soil_fertility_index'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3b715-cc26-4fd2-8d62-15f5826ba20b",
   "metadata": {},
   "source": [
    "## MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78d2267-b90a-45dd-b339-700c60f2a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression done\n",
    "from sklearn.tree import DecisionTreeRegressor done\n",
    "from sklearn.ensemble import RandomForestRegressor done\n",
    "from sklearn.ensemble import GradientBoostingRegressor done\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7fdae22-e2c0-4f6b-a888-e84b2beba8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regression algorithms\n",
    "linear_reg = LinearRegression()\n",
    "decision_tree_reg = DecisionTreeRegressor()\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "gradient_boosting_reg = GradientBoostingRegressor()\n",
    "mlp_reg = MLPRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb536697-cc23-4aa4-81e0-a898393d9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the W1 dataset\n",
    "X_train_w1, X_test_w1, y_train_w1, y_test_w1 = train_test_split(selected_features_w1, w1_data['water_utilization_efficiency'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the D1 dataset\n",
    "X_train_d1, X_test_d1, y_train_d1, y_test_d1 = train_test_split(selected_features_d1, d1_data['yield_per_area'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the D2 dataset\n",
    "X_train_d2, X_test_d2, y_train_d2, y_test_d2 = train_test_split(selected_features_d2, d2_data['soil_fertility_index'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0ab56ab-9210-4bbd-a652-c06140b4f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Score: 0.009651601648644648\n",
      "Decision Tree Score: -0.042156958051176474\n",
      "Random Forest Score: 0.9999935358461886\n",
      "Gradient Boosting Score: 0.08559191595866378\n",
      "MLP Score: 0.05198826992840189\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already split your datasets into training and testing sets\n",
    "# Replace X_train, y_train, X_test, and y_test with your actual data\n",
    "\n",
    "# Fit the models to the training data\n",
    "linear_reg.fit(X_train_w1, y_train_w1)\n",
    "decision_tree_reg.fit(X_train_d1, y_train_d1)\n",
    "random_forest_reg.fit(X_train_d2, y_train_d2)\n",
    "gradient_boosting_reg.fit(X_train_w1, y_train_w1)  # Assuming you're using the same dataset for this model\n",
    "mlp_reg.fit(X_train_d1, y_train_d1)  # Assuming you're using the same dataset for this model\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "linear_reg_score = linear_reg.score(X_test_w1, y_test_w1)\n",
    "decision_tree_score = decision_tree_reg.score(X_test_d1, y_test_d1)\n",
    "random_forest_score = random_forest_reg.score(X_test_d2, y_test_d2)\n",
    "gradient_boosting_score = gradient_boosting_reg.score(X_test_w1, y_test_w1)  # Assuming you're using the same dataset for this model\n",
    "mlp_score = mlp_reg.score(X_test_d1, y_test_d1)  # Assuming you're using the same dataset for this model\n",
    "\n",
    "# Print the model scores\n",
    "print(\"Linear Regression Score:\", linear_reg_score)\n",
    "print(\"Decision Tree Score:\", decision_tree_score)\n",
    "print(\"Random Forest Score:\", random_forest_score)\n",
    "print(\"Gradient Boosting Score:\", gradient_boosting_score)\n",
    "print(\"MLP Score:\", mlp_score)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "583f6ad6-c5a5-4941-aba4-540c5182f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Best Score for Random Forest: 0.9999893638324494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid_rf, cv=5)\n",
    "\n",
    "# Fit GridSearchCV to training data\n",
    "grid_search_rf.fit(X_train_d2, y_train_d2)\n",
    "\n",
    "# Get best parameters and best score\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "print(\"Best Parameters for Random Forest:\", best_params_rf)\n",
    "print(\"Best Score for Random Forest:\", best_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aadc0f-ca9d-403f-b8f1-6dc02ec7e2db",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Split the W1 dataset\n",
    "X_train_w1, X_test_w1, y_train_w1, y_test_w1 = train_test_split(selected_features_w1, w1_data['water_utilization_efficiency'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the D1 dataset\n",
    "X_train_d1, X_test_d1, y_train_d1, y_test_d1 = train_test_split(selected_features_d1, d1_data['yield_per_area'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the D2 dataset\n",
    "X_train_d2, X_test_d2, y_train_d2, y_test_d2 = train_test_split(selected_features_d2, d2_data['soil_fertility_index'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameters grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=random_forest_reg, param_grid=param_grid_rf, cv=5)\n",
    "\n",
    "# Fit GridSearchCV to training data for W1 dataset\n",
    "grid_search_rf.fit(X_train_w1, y_train_w1)\n",
    "\n",
    "# Get best parameters and best score for W1 dataset\n",
    "best_params_rf_w1 = grid_search_rf.best_params_\n",
    "best_score_rf_w1 = grid_search_rf.best_score_\n",
    "\n",
    "print(\"Best Parameters for Random Forest (W1 dataset):\", best_params_rf_w1)\n",
    "print(\"Best Score for Random Forest (W1 dataset):\", best_score_rf_w1)\n",
    "\n",
    "# Fit GridSearchCV to training data for D1 dataset\n",
    "grid_search_rf.fit(X_train_d1, y_train_d1)\n",
    "\n",
    "# Get best parameters and best score for D1 dataset\n",
    "best_params_rf_d1 = grid_search_rf.best_params_\n",
    "best_score_rf_d1 = grid_search_rf.best_score_\n",
    "\n",
    "print(\"Best Parameters for Random Forest (D1 dataset):\", best_params_rf_d1)\n",
    "print(\"Best Score for Random Forest (D1 dataset):\", best_score_rf_d1)\n",
    "\n",
    "# Fit GridSearchCV to training data for D2 dataset\n",
    "grid_search_rf.fit(X_train_d2, y_train_d2)\n",
    "\n",
    "# Get best parameters and best score for D2 dataset\n",
    "best_params_rf_d2 = grid_search_rf.best_params_\n",
    "best_score_rf_d2 = grid_search_rf.best_score_\n",
    "\n",
    "print(\"Best Parameters for Random Forest (D2 dataset):\", best_params_rf_d2)\n",
    "print(\"Best Score for Random Forest (D2 dataset):\", best_score_rf_d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0346fd54-16de-45bf-bad2-1ae325f6db91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Score: 0.9999926578023778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessing_steps = [('scaler', StandardScaler())]\n",
    "\n",
    "# Define modeling steps\n",
    "modeling_steps = [('regressor', RandomForestRegressor())]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=preprocessing_steps + modeling_steps)\n",
    "\n",
    "# Fit pipeline to training data\n",
    "pipeline.fit(X_train_d2, y_train_d2)\n",
    "\n",
    "# Evaluate pipeline on test data\n",
    "pipeline_score = pipeline.score(X_test_d2, y_test_d2)\n",
    "\n",
    "print(\"Pipeline Score:\", pipeline_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de874de-a233-48ae-b19d-bf165116fa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
